{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaadukanna/Chat-with-PDF-Chatbot/blob/main/kafka_pyspark_mongo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFX69MdB0zpZ",
        "outputId": "5430e24b-fb43-4ce2-ea25-31c9ff0f7a9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.1.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285388 sha256=4f70f06f4aea124bed61011ea081b1a538d6bfe336d73662576fcc9070dfa58f\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/77/a3/ff2f74cc9ab41f8f594dabf0579c2a7c6de920d584206e0834\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.1\n"
          ]
        }
      ],
      "source": [
        "# Install pyspark\n",
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "WFqLrDsH1JQQ",
        "outputId": "1e544dac-a6e5-4e6f-909c-e6ea4084292a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://440107d07693:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.4.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x79ad62bed150>"
            ]
          },
          "execution_count": 79,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Import SparkSession\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a Spark Session\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "\n",
        "# Check Spark Session Information\n",
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7tLahS9X1Pv3"
      },
      "outputs": [],
      "source": [
        "# Import a Spark function from library\n",
        "from pyspark.sql.functions import col"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqXchCke2ppH",
        "outputId": "f9de673c-6c2c-41f9-e610-3367da42fac6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting kafka-python\n",
            "  Downloading kafka_python-2.0.2-py2.py3-none-any.whl (246 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/246.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.7/246.5 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m246.5/246.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: kafka-python\n",
            "Successfully installed kafka-python-2.0.2\n"
          ]
        }
      ],
      "source": [
        "# install kafka-python\n",
        "!pip install kafka-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3qe_Vvz29zf"
      },
      "outputs": [],
      "source": [
        "# Download latest stable kafka archive\n",
        "!curl -sSOL https://downloads.apache.org/kafka/3.5.1/kafka_2.13-3.5.1.tgz\n",
        "!tar -xzf kafka_2.13-3.5.1.tgz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0M85c-PN3YX_",
        "outputId": "bce50736-02d5-43d7-9c6c-69fe29b3b264"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Waiting for 10 secs until kafka and zookeeper services are up and running\n"
          ]
        }
      ],
      "source": [
        "!./kafka_2.13-3.5.1/bin/zookeeper-server-start.sh -daemon ./kafka_2.13-3.5.1/config/zookeeper.properties\n",
        "!./kafka_2.13-3.5.1/bin/kafka-server-start.sh -daemon ./kafka_2.13-3.5.1/config/server.properties\n",
        "!echo \"Waiting for 10 secs until kafka and zookeeper services are up and running\"\n",
        "!sleep 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHE8xi483rr3",
        "outputId": "ba9e8e1d-c061-47f0-9c24-333c375745c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root        3376       1  0 08:59 ?        00:00:11 java -Xmx512M -Xms512M -server -XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:+ExplicitGCInvokesConcurrent -XX:MaxInlineLevel=15 -Djava.awt.headless=true -Xlog:gc*:file=/content/kafka_2.13-3.5.1/bin/../logs/zookeeper-gc.log:time,tags:filecount=10,filesize=100M -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dkafka.logs.dir=/content/kafka_2.13-3.5.1/bin/../logs -Dlog4j.configuration=file:./kafka_2.13-3.5.1/bin/../config/log4j.properties -cp /content/kafka_2.13-3.5.1/bin/../libs/activation-1.1.1.jar:/content/kafka_2.13-3.5.1/bin/../libs/aopalliance-repackaged-2.6.1.jar:/content/kafka_2.13-3.5.1/bin/../libs/argparse4j-0.7.0.jar:/content/kafka_2.13-3.5.1/bin/../libs/audience-annotations-0.13.0.jar:/content/kafka_2.13-3.5.1/bin/../libs/commons-cli-1.4.jar:/content/kafka_2.13-3.5.1/bin/../libs/commons-lang3-3.8.1.jar:/content/kafka_2.13-3.5.1/bin/../libs/connect-api-3.5.1.jar:/content/kafka_2.13-3.5.1/bin/../libs/connect-basic-auth-extension-3.5.1.jar:/content/kafka_2.13-3.5.1/bin/../libs/connect-json-3.5.1.jar:/content/kafka_2.13-3.5.1/bin/../libs/connect-mirror-3.5.1.jar:/content/kafka_2.13-3.5.1/bin/../libs/connect-mirror-client-3.5.1.jar:/content/kafka_2.13-3.5.1/bin/../libs/connect-runtime-3.5.1.jar:/content/kafka_2.13-3.5.1/bin/../libs/connect-transforms-3.5.1.jar:/content/kafka_2.13-3.5.1/bin/../libs/hk2-api-2.6.1.jar:/content/kafka_2.13-3.5.1/bin/../libs/hk2-locator-2.6.1.jar:/content/kafka_2.13-3.5.1/bin/../libs/hk2-utils-2.6.1.jar:/content/kafka_2.13-3.5.1/bin/../libs/jackson-annotations-2.13.5.jar:/content/kafka_2.13-3.5.1/bin/../libs/jackson-core-2.13.5.jar:/content/kafka_2.13-3.5.1/bin/../libs/jackson-databind-2.13.5.jar:/content/kafka_2.13-3.5.1/bin/../libs/jackson-dataformat-csv-2.13.5.jar:/content/kafka_2.13-3.5.1/bin/../libs/jackson-datatype-jdk8-2.13.5.jar:/content/kafka_2.13-3.5.1/bin/../libs/jackson-jaxrs-base-2.13.5.jar:/content/kafka_2.13-3.5.1/bin/../libs/jackson-jaxrs-json-provider-2.13.5.jar:/content/kafka_2.13-3.5.1/bin/../libs/jackson-module-jaxb-annotations-2.13.5.jar:/content/kafka_2.13-3.5.1/bin/../libs/jackson-module-scala_2.13-2.13.5.jar:/content/kafka_2.13-3.5.1/bin/../libs/jakarta.activation-api-1.2.2.jar:/content/kafka_2.13-3.5.1/bin/../libs/jakarta.annotation-api-1.3.5.jar:/content/kafka_2.13-3.5.1/bin/../libs/jakarta.inject-2.6.1.jar:/content/kafka_2.13-3.5.1/bin/../libs/jakarta.validation-api-2.0.2.jar:/content/kafka_2.13-3.5.1/bin/../libs/jakarta.ws.rs-api-2.1.6.jar:/content/kafka_2.13-3.5.1/bin/../libs/jakarta.xml.bind-api-2.3.3.jar:/content/kafka_2.13-3.5.1/bin/../libs/javassist-3.29.2-GA.jar:/content/kafka_2.13-3.5.1/bin/../libs/javax.activation-api-1.2.0.jar:/content/kafka_2.13-3.5.1/bin/../libs/javax.annotation-api-1.3.2.jar:/content/kafka_2.13-3.5.1/bin/../libs/javax.servlet-api-3.1.0.jar:/content/kafka_2.13-3.5.1/bin/../libs/javax.ws.rs-api-2.1.1.jar:/content/kafka_2.13-3.5.1/bin/../libs/jaxb-api-2.3.1.jar:/content/kafka_2.13-3.5.1/bin/../libs/jersey-client-2.39.1.jar:/content/kafka_2.13-3.5.1/bin/../libs/jersey-common-2.39.1.jar:/content/kafka_2.13-3.5.1/bin/../libs/jersey-container-servlet-2.39.1.jar:/content/kafka_2.13-3.5.1/bin/../libs/jersey-container-servlet-core-2.39.1.jar:/content/kafka_2.13-3.5.1/bin/../libs/jersey-hk2-2.39.1.jar:/content/kafka_2.13-3.5.1/bin/../libs/jersey-server-2.39.1.jar:/content/kafka_2.13-3.5.1/bin/../libs/jetty-client-9.4.51.v20230217.jar:/content/kafka_2.13-3.5.1/bin/../libs/jetty-continuation-9.4.51.v20230217.jar:/content/kafka_2.13-3.5.1/bin/../libs/jetty-http-9.4.51.v20230217.jar:/content/kafka_2.13-3.5.1/bin/../libs/jetty-io-9.4.51.v20230217.jar:/content/kafka_2.13-3.5.1/bin/../libs/jetty-security-9.4.51.v20230217.jar:/content/kafka_2.13-3.5.1/bin/../libs/jetty-server-9.4.51.v20230217.jar:/content/kafka_2.13-3.5.1/bin/../libs/jetty-servlet-9.4.51.v20230217.jar:/content/kafka_2.13-3.5.1/bin/../libs/jetty-servlets-9.4.51.v20230217.jar:/content/kafka_2.13-3.5.1/bin/../libs/jetty-util-9.4.51.v20230217.jar:/content/kafka_2.13-3.5.1/bin/../libs/jetty-util-ajax-9.4.51.v20230217.jar:/content/kafka_2.13-3.5.1/bin/../libs/jline-3.22.0.jar:/content/kafka_2.13-3.5.1/bin/../libs/jopt-simple-5.0.4.jar:/content/kafka_2.13-3.5.1/bin/../libs/jose4j-0.9.3.jar:/content/kafka_2.13-3.5.1/bin/../libs/kafka_2.13-3.5.1.jar:/content/kafka_2.13-3.5.1/bin/../libs/kafka-clients-3.5.1.jar:/content/kafka_2.13-3.5.1/bin/../libs/kafka-group-coordinator-3.5.1.jar:/content/kafka_2.13-3.5.1/bin/../libs/kafka-log4j-appender-3.5.1.jar:/content/kafka_2.13-3.5.1/bin/../libs/kafka-metadata-3.5.1.jar:/content/kafka_2.13-3.5.1/bin/../libs/kafka-raft-3.5.1.jar:/content/kafka_2.13-3.5.1/bin/../libs/kafka-server-common-3.5.1.jar:/content/kafka_2.13-3.5.1/bin/../libs/kafka-shell-3.5.1.jar:/content/kafka_2.13-3.5.1/bin/../libs/kafka-storage-3.5.1.jar:/content/kafka_2.13-3.5.1/bin/../libs/kafka-storage-api-3.5.1.jar:/content/kafka_2.13-3.5.1/bin/../libs/kafka-streams-3.5.1.jar:/content/kafka_2.13-3.5.1/bin/../libs/kafka-streams-examples-3.5.1.jar:/content/kafka_2.13-3.5.1/bin/../libs/kafka-streams-scala_2.13-3.5.1.jar:/content/kafka_2.13-3.5.1/bin/../libs/kafka-streams-test-utils-3.5.1.jar:/content/kafka_2.13-3.5.1/bin/../libs/kafka-tools-3.5.1.jar:/content/kafka_2.13-3.5.1/bin/../libs/kafka-tools-api-3.5.1.jar:/content/kafka_2.13-3.5.1/bin/../libs/lz4-java-1.8.0.jar:/content/kafka_2.13-3.5.1/bin/../libs/maven-artifact-3.8.8.jar:/content/kafka_2.13-3.5.1/bin/../libs/metrics-core-2.2.0.jar:/content/kafka_2.13-3.5.1/bin/../libs/metrics-core-4.1.12.1.jar:/content/kafka_2.13-3.5.1/bin/../libs/netty-buffer-4.1.94.Final.jar:/content/kafka_2.13-3.5.1/bin/../libs/netty-codec-4.1.94.Final.jar:/content/kafka_2.13-3.5.1/bin/../libs/netty-common-4.1.94.Final.jar:/content/kafka_2.13-3.5.1/bin/../libs/netty-handler-4.1.94.Final.jar:/content/kafka_2.13-3.5.1/bin/../libs/netty-resolver-4.1.94.Final.jar:/content/kafka_2.13-3.5.1/bin/../libs/netty-transport-4.1.94.Final.jar:/content/kafka_2.13-3.5.1/bin/../libs/netty-transport-classes-epoll-4.1.94.Final.jar:/content/kafka_2.13-3.5.1/bin/../libs/netty-transport-native-epoll-4.1.94.Final.jar:/content/kafka_2.13-3.5.1/bin/../libs/netty-transport-native-unix-common-4.1.94.Final.jar:/content/kafka_2.13-3.5.1/bin/../libs/osgi-resource-locator-1.0.3.jar:/content/kafka_2.13-3.5.1/bin/../libs/paranamer-2.8.jar:/content/kafka_2.13-3.5.1/bin/../libs/plexus-utils-3.3.1.jar:/content/kafka_2.13-3.5.1/bin/../libs/reflections-0.9.12.jar:/content/kafka_2.13-3.5.1/bin/../libs/reload4j-1.2.25.jar:/content/kafka_2.13-3.5.1/bin/../libs/rocksdbjni-7.1.2.jar:/content/kafka_2.13-3.5.1/bin/../libs/scala-collection-compat_2.13-2.10.0.jar:/content/kafka_2.13-3.5.1/bin/../libs/scala-java8-compat_2.13-1.0.2.jar:/content/kafka_2.13-3.5.1/bin/../libs/scala-library-2.13.10.jar:/content/kafka_2.13-3.5.1/bin/../libs/scala-logging_2.13-3.9.4.jar:/content/kafka_2.13-3.5.1/bin/../libs/scala-reflect-2.13.10.jar:/content/kafka_2.13-3.5.1/bin/../libs/slf4j-api-1.7.36.jar:/content/kafka_2.13-3.5.1/bin/../libs/slf4j-reload4j-1.7.36.jar:/content/kafka_2.13-3.5.1/bin/../libs/snappy-java-1.1.10.1.jar:/content/kafka_2.13-3.5.1/bin/../libs/swagger-annotations-2.2.8.jar:/content/kafka_2.13-3.5.1/bin/../libs/trogdor-3.5.1.jar:/content/kafka_2.13-3.5.1/bin/../libs/zookeeper-3.6.4.jar:/content/kafka_2.13-3.5.1/bin/../libs/zookeeper-jute-3.6.4.jar:/content/kafka_2.13-3.5.1/bin/../libs/zstd-jni-1.5.5-1.jar org.apache.zookeeper.server.quorum.QuorumPeerMain ./kafka_2.13-3.5.1/config/zookeeper.properties\n",
            "root        3739       1  0 08:59 ?        00:01:11 java -Xmx1G -Xms1G -server -XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:+ExplicitGCInvokesConcurrent -XX:MaxInlineLevel=15 -Djava.awt.headless=true -Xlog:gc*:file=/content/kafka_2.13-3.5.1/bin/../logs/kafkaServer-gc.log:time,tags:filecount=10,filesize=100M -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dkafka.logs.dir=/content/kafka_2.13-3.5.1/bin/../logs -Dlog4j.configuration=file:./kafka_2.13-3.5.1/bin/../config/log4j.properties -cp /content/kafka_2.13-3.5.1/bin/../libs/activation-1.1.1.jar:/content/kafka_2.13-3.5.1/bin/../libs/aopalliance-repackaged-2.6.1.jar:/content/kafka_2.13-3.5.1/bin/../libs/argparse4j-0.7.0.jar:/content/kafka_2.13-3.5.1/bin/../libs/audience-annotations-0.13.0.jar:/content/kafka_2.13-3.5.1/bin/../libs/commons-cli-1.4.jar:/content/kafka_2.13-3.5.1/bin/../libs/commons-lang3-3.8.1.jar:/content/kafka_2.13-3.5.1/bin/../libs/connect-api-3.5.1.jar:/content/kafka_2.13-3.5.1/bin/../libs/connect-basic-auth-extension-3.5.1.jar:/content/kafka_2.13-3.5.1/bin/../libs/connect-json-3.5.1.jar:/content/kafka_2.13-3.5.1/bin/../libs/connect-mirror-3.5.1.jar:/content/kafka_2.13-3.5.1/bin/../libs/connect-mirror-client-3.5.1.jar:/content/kafka_2.13-3.5.1/bin/../libs/connect-runtime-3.5.1.jar:/content/kafka_2.13-3.5.1/bin/../libs/connect-transforms-3.5.1.jar:/content/kafka_2.13-3.5.1/bin/../libs/hk2-api-2.6.1.jar:/content/kafka_2.13-3.5.1/bin/../libs/hk2-locator-2.6.1.jar:/content/kafka_2.13-3.5.1/bin/../libs/hk2-utils-2.6.1.jar:/content/kafka_2.13-3.5.1/bin/../libs/jackson-annotations-2.13.5.jar:/content/kafka_2.13-3.5.1/bin/../libs/jackson-core-2.13.5.jar:/content/kafka_2.13-3.5.1/bin/../libs/jackson-databind-2.13.5.jar:/content/kafka_2.13-3.5.1/bin/../libs/jackson-dataformat-csv-2.13.5.jar:/content/kafka_2.13-3.5.1/bin/../libs/jackson-datatype-jdk8-2.13.5.jar:/content/kafka_2.13-3.5.1/bin/../libs/jackson-jaxrs-base-2.13.5.jar:/content/kafka_2.13-3.5.1/bin/../libs/jackson-jaxrs-json-provider-2.13.5.jar:/content/kafka_2.13-3.5.1/bin/../libs/jackson-module-jaxb-annotations-2.13.5.jar:/content/kafka_2.13-3.5.1/bin/../libs/jackson-module-scala_2.13-2.13.5.jar:/content/kafka_2.13-3.5.1/bin/../libs/jakarta.activation-api-1.2.2.jar:/content/kafka_2.13-3.5.1/bin/../libs/jakarta.annotation-api-1.3.5.jar:/content/kafka_2.13-3.5.1/bin/../libs/jakarta.inject-2.6.1.jar:/content/kafka_2.13-3.5.1/bin/../libs/jakarta.validation-api-2.0.2.jar:/content/kafka_2.13-3.5.1/bin/../libs/jakarta.ws.rs-api-2.1.6.jar:/content/kafka_2.13-3.5.1/bin/../libs/jakarta.xml.bind-api-2.3.3.jar:/content/kafka_2.13-3.5.1/bin/../libs/javassist-3.29.2-GA.jar:/content/kafka_2.13-3.5.1/bin/../libs/javax.activation-api-1.2.0.jar:/content/kafka_2.13-3.5.1/bin/../libs/javax.annotation-api-1.3.2.jar:/content/kafka_2.13-3.5.1/bin/../libs/javax.servlet-api-3.1.0.jar:/content/kafka_2.13-3.5.1/bin/../libs/javax.ws.rs-api-2.1.1.jar:/content/kafka_2.13-3.5.1/bin/../libs/jaxb-api-2.3.1.jar:/content/kafka_2.13-3.5.1/bin/../libs/jersey-client-2.39.1.jar:/content/kafka_2.13-3.5.1/bin/../libs/jersey-common-2.39.1.jar:/content/kafka_2.13-3.5.1/bin/../libs/jersey-container-servlet-2.39.1.jar:/content/kafka_2.13-3.5.1/bin/../libs/jersey-container-servlet-core-2.39.1.jar:/content/kafka_2.13-3.5.1/bin/../libs/jersey-hk2-2.39.1.jar:/content/kafka_2.13-3.5.1/bin/../libs/jersey-server-2.39.1.jar:/content/kafka_2.13-3.5.1/bin/../libs/jetty-client-9.4.51.v20230217.jar:/content/kafka_2.13-3.5.1/bin/../libs/jetty-continuation-9.4.51.v20230217.jar:/content/kafka_2.13-3.5.1/bin/../libs/jetty-http-9.4.51.v20230217.jar:/content/kafka_2.13-3.5.1/bin/../libs/jetty-io-9.4.51.v20230217.jar:/content/kafka_2.13-3.5.1/bin/../libs/jetty-security-9.4.51.v20230217.jar:/content/kafka_2.13-3.5.1/bin/../libs/jetty-server-9.4.51.v20230217.jar:/content/kafka_2.13-3.5.1/bin/../libs/jetty-servlet-9.4.51.v20230217.jar:/content/kafka_2.13-3.5.1/bin/../libs/jetty-servlets-9.4.51.v20230217.jar:/content/kafka_2.13-3.5.1/bin/../libs/jetty-util-9.4.51.v20230217.jar:/content/kafka_2.13-3.5.1/bin/../libs/jetty-util-ajax-9.4.51.v20230217.jar:/content/kafka_2.13-3.5.1/bin/../libs/jline-3.22.0.jar:/content/kafka_2.13-3.5.1/bin/../libs/jopt-simple-5.0.4.jar:/content/kafka_2.13-3.5.1/bin/../libs/jose4j-0.9.3.jar:/content/kafka_2.13-3.5.1/bin/../libs/kafka_2.13-3.5.1.jar:/content/kafka_2.13-3.5.1/bin/../libs/kafka-clients-3.5.1.jar:/content/kafka_2.13-3.5.1/bin/../libs/kafka-group-coordinator-3.5.1.jar:/content/kafka_2.13-3.5.1/bin/../libs/kafka-log4j-appender-3.5.1.jar:/content/kafka_2.13-3.5.1/bin/../libs/kafka-metadata-3.5.1.jar:/content/kafka_2.13-3.5.1/bin/../libs/kafka-raft-3.5.1.jar:/content/kafka_2.13-3.5.1/bin/../libs/kafka-server-common-3.5.1.jar:/content/kafka_2.13-3.5.1/bin/../libs/kafka-shell-3.5.1.jar:/content/kafka_2.13-3.5.1/bin/../libs/kafka-storage-3.5.1.jar:/content/kafka_2.13-3.5.1/bin/../libs/kafka-storage-api-3.5.1.jar:/content/kafka_2.13-3.5.1/bin/../libs/kafka-streams-3.5.1.jar:/content/kafka_2.13-3.5.1/bin/../libs/kafka-streams-examples-3.5.1.jar:/content/kafka_2.13-3.5.1/bin/../libs/kafka-streams-scala_2.13-3.5.1.jar:/content/kafka_2.13-3.5.1/bin/../libs/kafka-streams-test-utils-3.5.1.jar:/content/kafka_2.13-3.5.1/bin/../libs/kafka-tools-3.5.1.jar:/content/kafka_2.13-3.5.1/bin/../libs/kafka-tools-api-3.5.1.jar:/content/kafka_2.13-3.5.1/bin/../libs/lz4-java-1.8.0.jar:/content/kafka_2.13-3.5.1/bin/../libs/maven-artifact-3.8.8.jar:/content/kafka_2.13-3.5.1/bin/../libs/metrics-core-2.2.0.jar:/content/kafka_2.13-3.5.1/bin/../libs/metrics-core-4.1.12.1.jar:/content/kafka_2.13-3.5.1/bin/../libs/netty-buffer-4.1.94.Final.jar:/content/kafka_2.13-3.5.1/bin/../libs/netty-codec-4.1.94.Final.jar:/content/kafka_2.13-3.5.1/bin/../libs/netty-common-4.1.94.Final.jar:/content/kafka_2.13-3.5.1/bin/../libs/netty-handler-4.1.94.Final.jar:/content/kafka_2.13-3.5.1/bin/../libs/netty-resolver-4.1.94.Final.jar:/content/kafka_2.13-3.5.1/bin/../libs/netty-transport-4.1.94.Final.jar:/content/kafka_2.13-3.5.1/bin/../libs/netty-transport-classes-epoll-4.1.94.Final.jar:/content/kafka_2.13-3.5.1/bin/../libs/netty-transport-native-epoll-4.1.94.Final.jar:/content/kafka_2.13-3.5.1/bin/../libs/netty-transport-native-unix-common-4.1.94.Final.jar:/content/kafka_2.13-3.5.1/bin/../libs/osgi-resource-locator-1.0.3.jar:/content/kafka_2.13-3.5.1/bin/../libs/paranamer-2.8.jar:/content/kafka_2.13-3.5.1/bin/../libs/plexus-utils-3.3.1.jar:/content/kafka_2.13-3.5.1/bin/../libs/reflections-0.9.12.jar:/content/kafka_2.13-3.5.1/bin/../libs/reload4j-1.2.25.jar:/content/kafka_2.13-3.5.1/bin/../libs/rocksdbjni-7.1.2.jar:/content/kafka_2.13-3.5.1/bin/../libs/scala-collection-compat_2.13-2.10.0.jar:/content/kafka_2.13-3.5.1/bin/../libs/scala-java8-compat_2.13-1.0.2.jar:/content/kafka_2.13-3.5.1/bin/../libs/scala-library-2.13.10.jar:/content/kafka_2.13-3.5.1/bin/../libs/scala-logging_2.13-3.9.4.jar:/content/kafka_2.13-3.5.1/bin/../libs/scala-reflect-2.13.10.jar:/content/kafka_2.13-3.5.1/bin/../libs/slf4j-api-1.7.36.jar:/content/kafka_2.13-3.5.1/bin/../libs/slf4j-reload4j-1.7.36.jar:/content/kafka_2.13-3.5.1/bin/../libs/snappy-java-1.1.10.1.jar:/content/kafka_2.13-3.5.1/bin/../libs/swagger-annotations-2.2.8.jar:/content/kafka_2.13-3.5.1/bin/../libs/trogdor-3.5.1.jar:/content/kafka_2.13-3.5.1/bin/../libs/zookeeper-3.6.4.jar:/content/kafka_2.13-3.5.1/bin/../libs/zookeeper-jute-3.6.4.jar:/content/kafka_2.13-3.5.1/bin/../libs/zstd-jni-1.5.5-1.jar kafka.Kafka ./kafka_2.13-3.5.1/config/server.properties\n",
            "root       45761     147  0 11:20 ?        00:00:00 /bin/bash -c ps -ef | grep kafka\n",
            "root       45763   45761  0 11:20 ?        00:00:00 grep kafka\n"
          ]
        }
      ],
      "source": [
        "!ps -ef | grep kafka"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XT1t1Xx535JH",
        "outputId": "1eceae5c-aec3-4815-8711-7f95217325a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING: Due to limitations in metric names, topics with a period ('.') or underscore ('_') could collide. To avoid issues it is best to use either, but not both.\n",
            "Created topic hello_topic.\n"
          ]
        }
      ],
      "source": [
        "# Create Kafka Topic\n",
        "!./kafka_2.13-3.5.1/bin/kafka-topics.sh --create --bootstrap-server 127.0.0.1:9092 --replication-factor 1 --partitions 1 --topic hello_topic\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hiO_OrWM4KB_",
        "outputId": "497cac41-e0f0-4a9e-9b64-b6011f67fd40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Topic: hello_topic\tTopicId: mej55sb6Qs22hv5dKOERWA\tPartitionCount: 1\tReplicationFactor: 1\tConfigs: \n",
            "\tTopic: hello_topic\tPartition: 0\tLeader: 0\tReplicas: 0\tIsr: 0\n"
          ]
        }
      ],
      "source": [
        "# Describe the topic\n",
        "!./kafka_2.13-3.5.1/bin/kafka-topics.sh --describe --bootstrap-server 127.0.0.1:9092 --topic hello_topic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "D5gOmtoWJILV",
        "outputId": "1578521d-1846-439e-9d18-18ddb0fa8ce7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-e963d62e-f84d-4bb0-b257-f987db52cc94\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>movieId</th>\n",
              "      <th>title</th>\n",
              "      <th>genres</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>12</td>\n",
              "      <td>Dracula: Dead and Loving It (1995)</td>\n",
              "      <td>Comedy|Horror</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>15</td>\n",
              "      <td>Cutthroat Island (1995)</td>\n",
              "      <td>Action|Adventure|Romance</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>17</td>\n",
              "      <td>Sense and Sensibility (1995)</td>\n",
              "      <td>Drama|Romance</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e963d62e-f84d-4bb0-b257-f987db52cc94')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e963d62e-f84d-4bb0-b257-f987db52cc94 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e963d62e-f84d-4bb0-b257-f987db52cc94');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-40e07d9c-d2b0-40ff-83ff-ccf1c05415bc\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-40e07d9c-d2b0-40ff-83ff-ccf1c05415bc')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const charts = await google.colab.kernel.invokeFunction(\n",
              "          'suggestCharts', [key], {});\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-40e07d9c-d2b0-40ff-83ff-ccf1c05415bc button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "    movieId                               title                    genres\n",
              "11       12  Dracula: Dead and Loving It (1995)             Comedy|Horror\n",
              "14       15             Cutthroat Island (1995)  Action|Adventure|Romance\n",
              "16       17        Sense and Sensibility (1995)             Drama|Romance"
            ]
          },
          "execution_count": 83,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"movies.csv\")\n",
        "df = df.iloc[[11, 14, 16]]\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sNcB9CdpJwZ9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from kafka import KafkaProducer\n",
        "# Initialize Kafka Producer\n",
        "producer = KafkaProducer(bootstrap_servers='127.0.0.1:9092')\n",
        "# Convert DataFrame to JSON and send to Kafka topic\n",
        "for _, row in df.iterrows():\n",
        "    producer.send('hello_topic', value=row.to_json().encode('utf-8'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8XcjxdUHO5M"
      },
      "outputs": [],
      "source": [
        "!./kafka_2.13-3.5.1/bin/kafka-console-producer.sh --bootstrap-server 127.0.0.1:9092 --topic data_topic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYwgaGwaFA_3",
        "outputId": "a8469d0b-6cdc-4bd7-9027-c789c7ae9bb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed a total of 9 messages\n"
          ]
        }
      ],
      "source": [
        "!./kafka_2.13-3.5.1/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic hello_topic --from-beginning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LFu5t2UnYgXu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WVJ5IGMpWyen",
        "outputId": "4fb6788e-12c4-48c0-b48b-b9735d9176cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-08-18 12:36:11--  https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.13/3.4.1/spark-sql-kafka-0-10_2.13-3.4.1.jar\n",
            "Resolving repo1.maven.org (repo1.maven.org)... 199.232.192.209, 199.232.196.209, 2a04:4e42:4c::209, ...\n",
            "Connecting to repo1.maven.org (repo1.maven.org)|199.232.192.209|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 435459 (425K) [application/java-archive]\n",
            "Saving to: ‘spark-sql-kafka-0-10_2.13-3.4.1.jar’\n",
            "\n",
            "\r          spark-sql   0%[                    ]       0  --.-KB/s               \rspark-sql-kafka-0-1 100%[===================>] 425.25K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2023-08-18 12:36:11 (9.73 MB/s) - ‘spark-sql-kafka-0-10_2.13-3.4.1.jar’ saved [435459/435459]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget \"https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.13/3.4.1/spark-sql-kafka-0-10_2.13-3.4.1.jar\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GfLbCNvaYhse",
        "outputId": "01209240-2ddd-45e4-9deb-c186f3dff562"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "kafka_2.13-3.5.1      movies.csv\n",
            "kafka_2.13-3.5.1.tgz  sample_data\n",
            "ml_ratings.csv\t      spark-streaming-kafka-0-8-assembly_2.11-2.4.8.jar\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "!ls\n",
        "#os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "#os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.8-bin-hadoop2.7\"\n",
        "os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars /content/spark-streaming-kafka-0-8-assembly_2.11-2.4.8.jar pyspark-shell'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "Np2PeWO6Y93l",
        "outputId": "f6cb70aa-9487-428f-dea9-dd552c1c5118"
      },
      "outputs": [
        {
          "ename": "AnalysisException",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-97-20d298d09b9e>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"kafka.bootstrap.servers\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"127.0.0.1:9092\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"subscribe\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"hello_topic\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Convert the value column from binary to string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/streaming/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     def json(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: Failed to find data source: kafka. Please deploy the application as per the deployment section of Structured Streaming + Kafka Integration Guide."
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import from_json\n",
        "from pyspark.sql.types import StructType, StringType, IntegerType\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"StreamingDataProcessing\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Define the schema for the streaming data\n",
        "schema = StructType().add(\"movieId\", IntegerType()).add(\"title\", StringType()).add(\"genres\", StringType())\n",
        "\n",
        "# Read streaming data from Kafka topic\n",
        "df = spark \\\n",
        "    .readStream \\\n",
        "    .format(\"kafka\") \\\n",
        "    .option(\"kafka.bootstrap.servers\", \"127.0.0.1:9092\") \\\n",
        "    .option(\"subscribe\", \"hello_topic\") \\\n",
        "    .load()\n",
        "\n",
        "# Convert the value column from binary to string\n",
        "df = df.withColumn(\"value\", df[\"value\"].cast(StringType()))\n",
        "\n",
        "# Parse the JSON data\n",
        "df = df.withColumn(\"data\", from_json(df[\"value\"], schema))\n",
        "\n",
        "# Select the required columns\n",
        "df = df.select(\"data.movieId\", \"data.title\", \"data.genres\")\n",
        "\n",
        "# Write the processed data to the console\n",
        "query = df \\\n",
        "    .writeStream \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .format(\"console\") \\\n",
        "    .start()\n",
        "\n",
        "# Wait for the query to terminate\n",
        "query.awaitTermination()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZ2Nq7CUpRjm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HtPyp5eHZf3I"
      },
      "outputs": [],
      "source": [
        "!wget https://dlcdn.apache.org/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz\n",
        "!tar -xvf spark-3.4.1-bin-hadoop3.tgz\n",
        "!pip install findspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5vE3gt7ZHQX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.4.1-bin-hadoop3\"\n",
        "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.2.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.0 pyspark-shell'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJpQ30zrtGVv"
      },
      "outputs": [],
      "source": [
        "\n",
        "!./spark-3.4.1-bin-hadoop3/bin/spark-submit --jars /content/spark-streaming-kafka-0-8-assembly_2.11-2.4.8.jar --master spark://127.0.0.1:7077 --deploy-mode client ./demospark.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "veyaSLD5uiAz",
        "outputId": "b683e822-1035-4f50-8f4a-6e0fbcf1769b"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python 3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0] on linux\n",
            "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "23/08/18 12:59:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "23/08/18 12:59:57 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
            "Welcome to\n",
            "      ____              __\n",
            "     / __/__  ___ _____/ /__\n",
            "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
            "   /__ / .__/\\_,_/_/ /_/\\_\\   version 3.4.1\n",
            "      /_/\n",
            "\n",
            "Using Python version 3.10.12 (main, Jun 11 2023 05:26:28)\n",
            "Spark context Web UI available at http://440107d07693:4041\n",
            "Spark context available as 'sc' (master = local[*], app id = local-1692363597686).\n",
            "SparkSession available as 'spark'.\n",
            ">>> "
          ]
        }
      ],
      "source": [
        "!./spark-3.4.1-bin-hadoop3/bin/pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4bMnwgpu1SJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "536b_05FtaAQ",
        "outputId": "30ef8274-9689-4e22-c0f3-02a80e3f48b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "beeline\t\t      pyspark\t\tspark-class.cmd      spark-shell.cmd\n",
            "beeline.cmd\t      pyspark2.cmd\tspark-connect-shell  spark-sql\n",
            "docker-image-tool.sh  pyspark.cmd\tsparkR\t\t     spark-sql2.cmd\n",
            "find-spark-home       run-example\tsparkR2.cmd\t     spark-sql.cmd\n",
            "find-spark-home.cmd   run-example.cmd\tsparkR.cmd\t     spark-submit\n",
            "load-spark-env.cmd    spark-class\tspark-shell\t     spark-submit2.cmd\n",
            "load-spark-env.sh     spark-class2.cmd\tspark-shell2.cmd     spark-submit.cmd\n"
          ]
        }
      ],
      "source": [
        "!ls /content/spark-3.4.1-bin-hadoop3/bin"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPkngX39lFvLX4L/KTgKsww",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}